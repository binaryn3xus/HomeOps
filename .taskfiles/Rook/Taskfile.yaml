---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

x-env: &env
  disk: "{{.disk}}"
  job: "{{.job}}"
  node: "{{.node}}"

vars:
  ROOK_SCRIPTS_DIR: "{{.ROOT_DIR}}/.taskfiles/Rook/scripts"
  ROOK_TEMPLATES_DIR: "{{.ROOT_DIR}}/.taskfiles/Rook/templates"

tasks:

  reset:
    desc: Reset Rook
    vars: &vars
      disk: "{{.cephdisk}}"
      node: "{{.ITEM}}"
    cmds:
      - for: { var: cephnodes }
        task: reset-data
        vars: *vars
      - for: { var: cephnodes }
        task: reset-disk
        vars: *vars
    requires:
      vars: ["cephnodes", "cephdisk"]

  reset-disk:
    desc: Reset a rook disk on a node
    prompt: Reset rook disk with '{{.node}}/{{.disk}}' in the cluster ... continue?
    summary: |
      Args:
        disk: Disk to wipe (required)
        node: Node the disk is on (required)
    cmds:
      - envsubst < <(cat {{.ROOK_TEMPLATES_DIR}}/WipeDiskJob.tmpl.yaml) | kubectl apply -f -
      - bash {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh {{.job}} default
      - kubectl -n default wait job/{{.job}} --for condition=complete --timeout=1m
      - kubectl -n default logs job/{{.job}}
      - kubectl -n default delete job {{.job}}
    env: *env
    requires:
      vars: ["disk", "node"]
    vars:
      job: wipe-disk-{{.node}}-{{.disk | replace "/" "-"}}
    preconditions:
      - test -f {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDiskJob.tmpl.yaml

  reset-data:
    desc: Reset rook data on a node
    prompt: Reset rook data on node '{{.node}}' in the '{{.cluster}}' cluster ... continue?
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        node: Node the data is on (required)
    cmds:
      - envsubst < <(cat {{.ROOK_TEMPLATES_DIR}}/WipeDataJob.tmpl.yaml) | kubectl --context {{.cluster}} apply -f -
      - bash {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh {{.job}} default {{.cluster}}
      - kubectl --context {{.cluster}} -n default wait job/{{.job}} --for condition=complete --timeout=1m
      - kubectl --context {{.cluster}} -n default logs job/{{.job}}
      - kubectl --context {{.cluster}} -n default delete job {{.job}}
    env: *env
    requires:
      vars: ["cluster", "node"]
    vars:
      job: wipe-data-{{.node}}
    preconditions:
      - test -f {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDataJob.tmpl.yaml

  archive-crashes:
    desc: Archive all Ceph crash reports using the rook-ceph-tools pod
    cmds:
      - |
        TOOLS_POD=$(kubectl get pods -n rook-ceph --no-headers -o custom-columns=":metadata.name" | grep "rook-ceph-tools")
        if [ -z "$TOOLS_POD" ]; then
          echo "rook-ceph-tools pod not found in the rook-ceph namespace."
          exit 1
        fi
        echo "[Found] rook-ceph-tools pod: $TOOLS_POD"
        echo "[Execute] 'ceph crash archive-all' in the rook-ceph-tools pod ..."
        kubectl exec -n rook-ceph -it "$TOOLS_POD" -- ceph crash archive-all
        echo "[Finished]"

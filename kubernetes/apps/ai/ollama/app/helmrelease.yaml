---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
  namespace: ai
spec:
  chartRef:
    kind: OCIRepository
    name: *app
  dependsOn: []
  interval: 1h
  values:
    controllers:
      ollama:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.13.5
            env:
              TZ: ${TIMEZONE}
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: /root/.ollama/models
            resources:
              requests:
                cpu: 200m
                memory: 1Gi
                nvidia.com/gpu: 1
              limits:
                memory: 12Gi
                nvidia.com/gpu: 1
        pod:
          nodeSelector:
            nvidia.feature.node.kubernetes.io/gpu: "true"
          runtimeClassName: nvidia

    service:
      app:
        type: LoadBalancer
        annotations:
          lbipam.cilium.io/ips: "10.0.30.44"
        ports:
          http:
            port: 11434

    persistence:
      models:
        existingClaim: ollama
        advancedMounts:
          ollama:
            app:
              - path: /root/.ollama
                subPath: .ollama
          sync-models:
            - path: /root/.ollama
              subPath: .ollama

      model-list:
        type: configMap
        name: ollama-models
        globalMounts:
          - path: /config/models.txt
            subPath: models.txt

      sync-script:
        type: configMap
        name: ollama-sync-script
        globalMounts:
          - path: /scripts/sync-models.sh
            subPath: sync-models.sh

      dev:
        type: hostPath
        hostPath: /dev/dri
        globalMounts:
          - readOnly: true

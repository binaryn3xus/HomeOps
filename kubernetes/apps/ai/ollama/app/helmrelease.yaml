---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
  namespace: ai
spec:
  chartRef:
    kind: OCIRepository
    name: *app
  interval: 1h
  values:
    controllers:
      ollama:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          pull-models:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.14.1
            command:
              - /bin/sh
              - -c
              - |
                echo "Starting Ollama server for model pulling..."
                ollama serve &
                OLLAMA_PID=$!

                # Wait for Ollama to be ready
                echo "Waiting for Ollama to start..."
                for i in $(seq 1 30); do
                  if ollama list >/dev/null 2>&1; then
                    echo "Ollama is ready!"
                    break
                  fi
                  sleep 2
                done

                # Pull required models
                echo "Pulling llama3.2..."
                ollama pull llama3.2:latest

                echo "Pulling nomic-embed-text for embeddings..."
                ollama pull nomic-embed-text

                echo "All models pulled successfully!"
                kill $OLLAMA_PID
                wait $OLLAMA_PID 2>/dev/null || true
            env:
              - name: OLLAMA_MODELS
                value: /models
              - name: OLLAMA_HOST
                value: 0.0.0.0
        containers:
          app:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.14.1
            env:
              TZ: ${TIMEZONE}
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: /models
              OLLAMA_KEEP_ALIVE: "5m"
              OLLAMA_NUM_PARALLEL: 1
              OLLAMA_CONTEXT_LENGTH: "4096"
              OLLAMA_MAX_LOADED_MODELS: 1
              OLLAMA_FLASH_ATTENTION: 1
            resources:
              requests:
                cpu: 200m
                memory: 1Gi
                nvidia.com/gpu: 1
              limits:
                memory: 12Gi
                nvidia.com/gpu: 1
    defaultPodOptions:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: sku
          operator: Equal
          value: gpu
          effect: NoSchedule
      runtimeClassName: nvidia

    service:
      app:
        type: LoadBalancer
        annotations:
          lbipam.cilium.io/ips: "10.0.30.44"
        ports:
          http:
            port: 11434

    persistence:
      models:
        existingClaim: ollama-openebs-pvc
        globalMounts:
          - path: /models
